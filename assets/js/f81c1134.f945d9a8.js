"use strict";(self.webpackChunkcurvine_doc=self.webpackChunkcurvine_doc||[]).push([[8130],{7735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/2025/11/27/observability","metadata":{"permalink":"/blog/2025/11/27/observability","editUrl":"https://github.com/curvineio/curvine-doc/edit/main/blog/blog/2025-11-27-observability/index.md","source":"@site/blog/2025-11-27-observability/index.md","title":"Observability Construction for Curvine","description":"Curvine as a high-performance distributed cache system has strict requirements for performance, stability, and reliability. To ensure the system maintains optimal performance under various load conditions and to quickly locate and resolve potential issues, we have built a comprehensive monitoring solution based on Prometheus and Grafana. This monitoring system provides deep observability capabilities for Master nodes, Worker nodes, Fuse nodes, and S3 Gateway, enabling real-time monitoring of cache cluster scale, operational status, performance metrics, and resource usage through the collection of key metrics from each component.","date":"2025-11-27T00:00:00.000Z","tags":[],"readingTime":6.15,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"nextItem":{"title":"Curvine Distributed Cache System User Guide","permalink":"/blog/2025/09/28/user-guide"}},"content":"Curvine as a high-performance distributed cache system has strict requirements for performance, stability, and reliability. To ensure the system maintains optimal performance under various load conditions and to quickly locate and resolve potential issues, we have built a comprehensive monitoring solution based on Prometheus and Grafana. This monitoring system provides deep observability capabilities for Master nodes, Worker nodes, Fuse nodes, and S3 Gateway, enabling real-time monitoring of cache cluster scale, operational status, performance metrics, and resource usage through the collection of key metrics from each component.\\n\\n## Monitoring Architecture\\n\\nThis monitoring system adopts the following core components:\\n\\n- **Prometheus**: Responsible for metric collection, storage, and querying\\n- **Grafana**: Provides data visualization and dashboard display\\n\\n## Observability Metrics\\n### Master Node Metrics\\n\\nAs the cluster\'s metadata management center, Master nodes provide the following key metrics:\\n\\n#### Capacity Metrics\\n\\nCapacity metrics are fundamental for evaluating system storage resource usage, crucial for capacity planning, resource optimization, and preventive maintenance. By monitoring these metrics, storage bottlenecks can be identified in a timely manner, capacity requirements can be predicted, and sufficient space can be ensured to handle business growth.\\n\\n![Capacity Metrics](./1.png)\\n| Metric Name | Description |\\n|-------------|-------------|\\n| inode_dir_num | Number of directories |\\n| inode_file_num | Number of files |\\n| num_blocks | Total number of blocks |\\n| blocks_size_avg | Average block size |\\n| capacity | Total storage capacity |\\n| available | Available storage space |\\n| fs_used | File system used space |\\n\\n#### Resource Metrics\\n\\nResource metrics reflect the system\'s usage of computing resources, significant for performance tuning, resource allocation, and fault prevention. Memory usage directly affects system performance and stability, especially for RocksDB as the core storage engine, whose memory usage needs precise monitoring to avoid memory overflow and performance degradation.\\n\\n![Resource Metrics](./2.png)\\n| Metric Name | Description |\\n|-------------|-------------|\\n| used_memory_bytes | Used memory in bytes |\\n| rocksdb_used_memory_bytes | RocksDB memory usage |\\n\\n#### Cluster Status Metrics\\n\\nCluster status metrics provide a real-time view of the overall system health, crucial for ensuring high availability and data consistency. By monitoring Worker node status and replication task execution, node failures, data inconsistencies, and other issues can be quickly identified, ensuring reliable operation of the distributed cache system.\\n\\n![Cluster Status](./3.png)\\n| Metric Name | Description |\\n|-------------|-------------|\\n| worker_num | Number of workers (classified by status) |\\n| replication_staging_number | Number of blocks waiting for replication |\\n| replication_inflight_number | Number of blocks currently being replicated |\\n| replication_failure_count | Total cumulative replication failures |\\n\\n#### Performance Metrics\\n\\nPerformance metrics are core indicators for measuring system responsiveness and processing efficiency, playing a key role in performance optimization and capacity planning. The total count and total time of RPC requests can be used to calculate average response time, directly reflecting system processing capability, while analysis of various operation durations helps identify performance bottlenecks and guide system optimization.\\n\\n![Performance Metrics](./4.png)\\n| Metric Name | Description |\\n|-------------|-------------|\\n| rpc_request_total_count | Total RPC request count |\\n| rpc_request_total_time | Total RPC request time |\\n| operation_duration | Operation duration (classified by type, excluding heartbeat) |\\n\\n#### Journal System Metrics\\n\\nThe Journal system is a key component for ensuring data consistency and fault recovery, and its performance directly affects system write performance and data reliability. Monitoring Journal queue length and flush performance can help identify write bottlenecks in a timely manner, prevent data loss risks, and ensure system stability in high-concurrency write scenarios.\\n\\n![Journal Metrics](./5.png)\\n| Metric Name | Description |\\n|-------------|-------------|\\n| journal_queue_len | Journal queue length |\\n| journal_flush_count | Journal flush count |\\n| journal_flush_time | Journal flush time |\\n\\n### Client Metrics (Fuse/S3 Gateway)\\n\\nFuse and S3 Gateway metrics are collected through Client\\n\\n#### Cache Metrics\\n\\nCache metrics directly reflect the core value of the cache system - improving access performance. Mount cache hit rate is a key indicator for measuring cache effectiveness, where high hit rates mean fewer backend accesses and faster response speeds. These metrics are crucial for evaluating cache strategy effectiveness and optimizing cache configuration.\\n\\n![Cache Metrics](./6.png)\\n| Metric Name | Description |\\n|-------------|-------------|\\n| client_mount_cache_hits | Mount cache hit count |\\n| client_mount_cache_misses | Mount cache miss count |\\n\\n#### I/O Metrics\\n\\nI/O metrics are core data for evaluating system read/write performance, providing guidance for performance tuning and capacity planning. By monitoring read/write bytes and duration, read/write throughput can be calculated, accurately assessing system I/O performance bottlenecks, optimizing storage strategies, and ensuring stable performance in high-concurrency access scenarios.\\n\\n![I/O Metrics](./7.png)\\n| Metric Name | Description |\\n|-------------|-------------|\\n| client_write_bytes | Write bytes |\\n| client_write_time_us | Write time (microseconds) |\\n| client_read_bytes | Read bytes |\\n| client_read_time_us | Read time (microseconds) |\\n\\n#### Metadata Operation Metrics\\n\\nMetadata operation performance directly affects file system response speed, crucial for improving user experience and overall system performance. Analysis of metadata operation duration helps identify metadata management bottlenecks, optimize directory structure, and improve file system operation efficiency.\\n\\n![Metadata Operation Metrics](./8.png)\\n| Metric Name | Description |\\n|-------------|-------------|\\n| client_metadata_operation_duration | Metadata operation duration |\\n\\n### Worker Node Metrics\\n\\nAs data storage nodes, Worker nodes provide comprehensive storage and performance metrics:\\n\\n#### Capacity Metrics\\n\\nWorker node capacity metrics are core to data storage management, playing a key role in load balancing, data migration, and capacity planning. By monitoring storage usage of each node, intelligent data distribution can be achieved, preventing single-point overload and ensuring optimal utilization of storage resources across the entire cache cluster.\\n\\n![Capacity Metrics](./9.png)\\n| Metric Name | Description |\\n|-------------|-------------|\\n| capacity | Total storage capacity |\\n| available | Available storage space |\\n| fs_used | File system used space |\\n| num_blocks | Total number of blocks |\\n| num_blocks_to_delete | Number of blocks to be deleted |\\n\\n#### I/O Metrics\\n\\nWorker node I/O metrics reflect the actual performance of data storage, crucial for evaluating storage hardware efficiency and optimizing data access patterns. Detailed read/write statistics help identify hot data, optimize data layout, and improve overall storage performance and response speed.\\n\\n![I/O Metrics](./10.png)\\n| Metric Name | Description |\\n|-------------|-------------|\\n| write_bytes | Write bytes |\\n| write_time_us | Write time (microseconds) |\\n| write_count | Write count |\\n| write_blocks | Write blocks (classified by type) |\\n| read_bytes | Read bytes |\\n| read_time_us | Read time (microseconds) |\\n| read_count | Read count |\\n| read_blocks | Read blocks (classified by type) |\\n\\n#### Resource Metrics\\n\\nWorker node resource usage directly affects the stability and performance of data storage services, significant for resource scheduling and performance optimization. Reasonable memory usage is the foundation for ensuring data cache efficiency and needs precise monitoring to avoid resource competition and performance degradation.\\n\\n![Resource Metrics](./11.png)\\n| Metric Name | Description |\\n|-------------|-------------|\\n| used_memory_bytes | Used memory in bytes |\\n\\n#### Hardware Status Metrics\\n\\nHardware status metrics are an important monitoring dimension for ensuring data reliability and system availability, crucial for preventive maintenance and rapid fault response. By monitoring disk health status in real-time, hardware failure risks can be identified early, enabling timely data migration and hardware replacement, ensuring data security and continuous availability of the cache system.\\n\\n![Hardware Status Metrics](./12.png)\\n| Metric Name | Description |\\n|-------------|-------------|\\n| failed_disks | Number of failed storage devices |\\n| total_disks | Total number of storage disks |\\n\\n## Global Dashboards\\n### Master\\n![Master](./master.png)\\n### Worker\\n![Worker](./worker.png)\\n### Client\\n![Client](./client.png)\\n\\n## Summary\\n\\nThe observability design of the Curvine distributed cache system covers the complete chain from metadata management to data storage, achieving the following through fine-grained metric collection:\\n\\n- **End-to-End Monitoring**: Complete monitoring from client requests to data storage, ensuring performance and status of each link are observable\\n- **Multi-Dimensional Observation**: Covering multiple dimensions including performance, capacity, and status, providing a comprehensive view of system health\\n- **Real-Time Alerting**: Real-time monitoring and alerting based on key metrics, enabling timely detection of anomalies and rapid response\\n- **Fault Diagnosis**: Detailed metric data supports rapid fault location, reducing fault recovery time\\n- **Performance Optimization**: Continuous monitoring and analysis provide data support for system performance tuning\\n- **Capacity Planning**: Based on historical trends and real-time data, providing decision-making basis for capacity expansion and resource optimization\\n\\nThrough this comprehensive monitoring system, Curvine can maintain high availability and high performance in complex distributed environments, providing users with stable and reliable cache services."},{"id":"/2025/09/28/user-guide","metadata":{"permalink":"/blog/2025/09/28/user-guide","editUrl":"https://github.com/curvineio/curvine-doc/edit/main/blog/blog/2025-09-28-user-guide/index.md","source":"@site/blog/2025-09-28-user-guide/index.md","title":"Curvine Distributed Cache System User Guide","description":"Version","date":"2025-09-28T00:00:00.000Z","tags":[],"readingTime":11.53,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Observability Construction for Curvine","permalink":"/blog/2025/11/27/observability"},"nextItem":{"title":"Building a Curvine Cluster from Scratch & FIO Testing","permalink":"/blog/2025/08/08/fio-bench"}},"content":"[![Version](https://img.shields.io/badge/version-v1.0-blue.svg)](https://github.com/curvine/curvine)\\n[![License](https://img.shields.io/badge/license-Apache%202.0-green.svg)](https://www.apache.org/licenses/LICENSE-2.0)\\n[![Documentation](https://img.shields.io/badge/docs-latest-brightgreen.svg)](https://docs.curvine.io)\\n\\n## \ud83d\udcda Table of Contents\\n\\n- [\ud83c\udfaf System Overview](#-system-overview)\\n- [\ud83d\udcc2 Path Mount Management](#-path-mount-management)\\n- [\ud83d\udcbe Intelligent Caching Strategies](#-intelligent-caching-strategies)\\n- [\ud83d\udd04 Data Consistency Guarantees](#-data-consistency-guarantees)\\n- [\ud83e\udd16 AI/ML Scenario Applications](#-aiml-scenario-applications)\\n- [\ud83d\uddc4\ufe0f Big Data Ecosystem Integration](#\ufe0f-big-data-ecosystem-integration)\\n- [\ud83d\udca1 Best Practices](#-best-practices)\\n- [\ud83c\udfaf Summary](#-summary)\\n\\n---\\n\\n## \ud83c\udfaf System Overview\\n\\nCurvine is a high-performance, cloud-native distributed caching system designed for modern data-intensive applications. It provides an intelligent caching layer between underlying storage (UFS) and compute engines, significantly improving data access performance.\\n\\n\\n### \ud83c\udfc6 Performance Advantages\\n\\nCompared to traditional storage access methods, Curvine can provide:\\n\\n| Metric | Cloud Storage | Curvine Cache | Performance Improvement |\\n|--------|---------------|---------------|-------------------------|\\n| **Read Latency** | 100-500ms | 1-10ms | **10-50x** |\\n| **Throughput** | 100-500 MB/s | 1-10 GB/s | **10-20x** |\\n| **IOPS** | 1K-10K | 100K-1M | **100x** |\\n| **Concurrent Connections** | 100-1K | 10K-100K | **100x** |\\n\\n---\\n\\n### Core Components\\n\\n- **Master Cluster**: Metadata management, cache scheduling, consistency guarantees\\n- **Worker Nodes**: Data caching, I/O processing, task execution\\n- **Client SDK**: Multi-language clients, supporting Rust, Fuse, Java, Python\\n- **Job Manager**: Distributed task scheduling and management\\n- **Metrics System**: Real-time monitoring and performance analysis\\n\\n---\\n\\n\\n## \ud83d\udcc2 Path Mount Management\\n\\nMounting is the first step in using Curvine cache, which establishes the mapping relationship between underlying storage (UFS) and cache paths.\\n\\n### Mounting Modes Explained\\n\\nCurvine supports two flexible mounting modes:\\n\\n#### \ud83c\udfaf CST Mode (Consistent Path Mode)\\n```bash\\n# Consistent path, easy to manage and maintain\\nbin/cv mount s3://bucket/data /bucket/data --mnt-type cst\\n```\\n\\n**Ideal scenarios**:\\n- Data lake scenarios with clear path structures\\n- Production environments requiring intuitive path mapping\\n- Data platforms with multi-team collaboration\\n\\n#### \ud83d\udd00 Arch Mode (Orchestration Mode)\\n```bash\\n# Flexible path mapping, supporting complex path transformations\\nbin/cv mount s3://complex-bucket/deep/nested/path /simple/data --mnt-type arch\\n```\\n\\n**Ideal scenarios**:\\n- Complex storage hierarchies\\n- Scenarios requiring path abstraction\\n- Multi-cloud storage unified access\\n\\n### Complete Mounting Example\\n\\n```bash\\n# Mount S3 storage to Curvine (production-grade configuration)\\nbin/cv mount \\\\\\ns3://bucket/warehouse/tpch_500g.db/orders \\\\\\n/bucket/warehouse/tpch_500g.db/orders \\\\\\n--ttl-ms 24h \\\\\\n--ttl-action delete \\\\\\n--replicas 3 \\\\\\n--block-size 128MB \\\\\\n--consistency-strategy always \\\\\\n--storage-type ssd \\\\\\n-c s3.endpoint_url=https://s3.ap-southeast-1.amazonaws.com \\\\\\n-c s3.credentials.access=access_key \\\\\\n-c s3.credentials.secret=secret_key \\\\\\n-c s3.region_name=ap-southeast-1 \\n```\\n\\n### Mounting Parameters Explained\\n\\n| Parameter | Type | Default | Description | Example |\\n|-----------|------|---------|-------------|---------|\\n| `--ttl-ms` | duration | `0` | Cache data expiration time | `24h`, `7d`, `30d` |\\n| `--ttl-action` | enum | `none` | Expiration policy: `delete`/`none` | `delete` |\\n| `--replicas` | int | `1` | Number of data replicas (1-5) | `3` |\\n| `--block-size` | size | `128MB` | Cache block size | `64MB`, `128MB`, `256MB` |\\n| `--consistency-strategy` | enum | `always` | Consistency strategy | `none`/`always`/`period` |\\n| `--storage-type` | enum | `disk` | Storage medium type | `mem`/`ssd`/`disk` |\\n\\n### Mount Point Management\\n\\n```bash\\n# View all mount points\\nbin/cv mount\\n\\n# Unmount path\\nbin/cv unmount /bucket/warehouse/tpch_500g.db/orders\\n```\\n\\n---\\n\\n## \ud83d\udcbe Intelligent Caching Strategies\\n\\nCurvine provides multiple intelligent caching strategies, from passive response to active prediction, comprehensively optimizing data access performance.\\n\\n### Active Data Preloading\\n\\nActive loading allows you to warm up the cache before business peaks to ensure optimal performance:\\n\\n```bash\\n# Basic loading\\nbin/cv load s3:/bucket/warehouse/critical-dataset\\n\\n# Synchronous loading with progress monitoring\\nbin/cv load s3:/bucket/warehouse/critical-dataset -w\\n\\n```\\n\\n### Automatic Caching Strategy\\n\\nCurvine\'s automatic caching system has significant advantages over traditional solutions:\\n\\n#### \u2728 Curvine Intelligent Cache Architecture\\n\\n![curvine](./curvine.png)\\n\\n#### Core Advantage Comparison\\n\\n| Feature | Open Source Competitors | Curvine | Advantage Description |\\n|---------|--------------------|---------|-----------------------|\\n| **Loading Granularity** | Block-level | File/Directory-level | Avoid fragmentation, ensure integrity |\\n| **Duplicate Processing** | Exists duplicate loading | Intelligent deduplication | Save bandwidth and storage resources |\\n| **Task Scheduling** | Simple queue | Distributed Job Manager | Efficient concurrency, load balancing |\\n| **Consistency Guarantee** | Passive checking | Active awareness | Real-time data synchronization |\\n\\n---\\n\\n## \ud83d\udd04 Data Consistency Guarantees\\n\\nData consistency is a core challenge for caching systems, and Curvine provides multi-level consistency guarantee mechanisms.\\n\\n### Consistency Strategy Details\\n\\n#### 1. \ud83d\udeab None Mode - Highest Performance\\n```bash\\nbin/cv mount s3://bucket/path /bucket/path --consistency-strategy=none\\n```\\n- **Ideal scenarios**: Static data, archived data, read-only datasets\\n- **Performance**: \u2b50\u2b50\u2b50\u2b50\u2b50 (fastest)\\n- **Consistency**: \u2b50\u2b50 (TTL-dependent)\\n\\n#### 2. \u2705 Always Mode - Strong Consistency\\n```bash\\nbin/cv mount s3://bucket/path /bucket/path --consistency-strategy=always\\n```\\n- **Ideal scenarios**: Frequently updated business data, critical business systems\\n- **Performance**: \u2b50\u2b50\u2b50 (has overhead)\\n- **Consistency**: \u2b50\u2b50\u2b50\u2b50\u2b50 (strong consistency)\\n\\n#### 3. \ud83d\udd70\ufe0f Period Mode - Balanced Solution\\n```bash\\nbin/cv mount s3://bucket/path /bucket/path \\\\\\n  --consistency-strategy=period \\\\\\n  --check-interval=5m\\n```\\n- **Ideal scenarios**: Data with predictable update frequency\\n- **Performance**: \u2b50\u2b50\u2b50\u2b50 (good)\\n- **Consistency**: \u2b50\u2b50\u2b50\u2b50 (periodically guaranteed)\\n\\n### Cache Performance Monitoring\\n\\nMonitoring cache hit ratio is an important way to evaluate the effectiveness of consistency strategies:\\n\\n```bash\\n# Get cache hit ratio\\ncurl -s http://master:9001/metrics | grep -E \\"(cache_hits|cache_misses)\\"\\n```\\n\\n```prometheus\\nclient_mount_cache_hits{id=\\"3108497238\\"} 823307\\nclient_mount_cache_misses{id=\\"3108497238\\"} 4380\\n```\\n\\n---\\n\\n## \ud83e\udd16 AI/ML Scenario Applications\\n\\nAI and machine learning workloads have extremely high requirements for storage performance, and Curvine provides specially optimized functions for this.\\n\\n### Deep Learning Training Optimization\\n\\n```bash\\n# Optimized data loading for GPU clusters\\nbin/cv mount s3://datasets/imagenet /datasets/imagenet \\\\\\n  --storage-type=mem \\\\\\n  --block-size=1GB \\\\\\n  --replicas=2 \\n\\n```\\n\\n### Model Serving Scenarios\\n\\n```bash\\n# Model file caching (low-latency access)\\nbin/cv mount s3://model/bert-large /models/bert-large \\\\\\n  --storage-type=mem \\\\\\n  --ttl-ms=none \\\\\\n  --consistency-strategy=always \\n\\n# Inference data caching\\nbin/cv mount s3://inference/input /inference/input \\\\\\n  --storage-type=ssd \\\\\\n  --ttl-ms=1h \\\\\\n  --consistency-strategy=none \\n```\\n\\n\\n### \ud83d\udd17 POSIX Semantics and FUSE Access\\n\\nCurvine perfectly supports POSIX semantics through the FUSE (Filesystem in Userspace) interface, allowing the Curvine cluster to be mounted as a local file system, providing a transparent file access experience for AI/ML applications.\\n\\n\\n#### FUSE Usage in AI/ML Training\\n\\n##### 1. Deep Learning Training Data Access\\n\\n```python\\n# PyTorch training script\\nimport torch\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom torchvision import transforms\\nfrom PIL import Image\\nimport os\\n\\nclass CurvineImageDataset(Dataset):\\n    def __init__(self, root_dir, transform=None):\\n        \\"\\"\\"\\n        Directly access data in Curvine through FUSE mount point\\n        root_dir: FUSE mount point path, such as /curvine-fuse/datasets/imagenet\\n        \\"\\"\\"\\n        self.root_dir = root_dir\\n        self.transform = transform\\n        self.image_paths = []\\n        \\n        # Directly traverse the FUSE-mounted directory\\n        for class_dir in os.listdir(root_dir):\\n            class_path = os.path.join(root_dir, class_dir)\\n            if os.path.isdir(class_path):\\n                for img_file in os.listdir(class_path):\\n                    if img_file.lower().endswith((\'.png\', \'.jpg\', \'.jpeg\')):\\n                        self.image_paths.append(os.path.join(class_path, img_file))\\n    \\n    def __len__(self):\\n        return len(self.image_paths)\\n    \\n    def __getitem__(self, idx):\\n        # Access data through standard file operations, enjoying Curvine cache acceleration\\n        img_path = self.image_paths[idx]\\n        image = Image.open(img_path).convert(\'RGB\')\\n        \\n        if self.transform:\\n            image = self.transform(image)\\n            \\n        # Extract label from path\\n        label = os.path.basename(os.path.dirname(img_path))\\n        return image, label\\n\\n# Usage example\\ntransform = transforms.Compose([\\n    transforms.Resize((224, 224)),\\n    transforms.ToTensor(),\\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \\n                        std=[0.229, 0.224, 0.225])\\n])\\n\\n# Directly use the path of the FUSE mount point\\ndataset = CurvineImageDataset(\\n    root_dir=\'/curvine-fuse/datasets/imagenet/train\',\\n    transform=transform\\n)\\n\\ndataloader = DataLoader(\\n    dataset, \\n    batch_size=64, \\n    shuffle=True, \\n    num_workers=8,\\n    pin_memory=True\\n)\\n\\n# Training loop\\nfor epoch in range(num_epochs):\\n    for batch_idx, (data, targets) in enumerate(dataloader):\\n        # Data is automatically loaded from Curvine cache through FUSE\\n        # Enjoy near-memory access speed\\n        outputs = model(data.cuda())\\n        loss = criterion(outputs, targets.cuda())\\n        # ... training logic\\n```\\n\\n##### 2. TensorFlow/Keras Data Pipeline\\n\\n```python\\nimport tensorflow as tf\\nimport os\\n\\ndef create_curvine_dataset(data_dir, batch_size=32):\\n    \\"\\"\\"\\n    Create TensorFlow data pipeline through FUSE mount point\\n    data_dir: FUSE-mounted data directory\\n    \\"\\"\\"\\n    \\n    # Directly access FUSE-mounted data using standard file APIs\\n    def load_and_preprocess_image(path):\\n        # TensorFlow transparently accesses Curvine cache through FUSE\\n        image = tf.io.read_file(path)\\n        image = tf.image.decode_jpeg(image, channels=3)\\n        image = tf.image.resize(image, [224, 224])\\n        image = tf.cast(image, tf.float32) / 255.0\\n        return image\\n    \\n    # Scan files in the FUSE-mounted directory\\n    image_paths = []\\n    labels = []\\n    \\n    for class_name in os.listdir(data_dir):\\n        class_dir = os.path.join(data_dir, class_name)\\n        if os.path.isdir(class_dir):\\n            for img_file in os.listdir(class_dir):\\n                if img_file.lower().endswith((\'.png\', \'.jpg\', \'.jpeg\')):\\n                    image_paths.append(os.path.join(class_dir, img_file))\\n                    labels.append(class_name)\\n    \\n    # Create dataset\\n    path_ds = tf.data.Dataset.from_tensor_slices(image_paths)\\n    label_ds = tf.data.Dataset.from_tensor_slices(labels)\\n    \\n    # Apply preprocessing\\n    image_ds = path_ds.map(\\n        load_and_preprocess_image, \\n        num_parallel_calls=tf.data.AUTOTUNE\\n    )\\n    \\n    # Combine data and labels\\n    dataset = tf.data.Dataset.zip((image_ds, label_ds))\\n    \\n    return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\\n\\n# Usage example\\ntrain_dataset = create_curvine_dataset(\'/curvine-fuse/datasets/imagenet/train\')\\nval_dataset = create_curvine_dataset(\'/curvine-fuse/datasets/imagenet/val\')\\n\\n# Model training\\nmodel.fit(\\n    train_dataset,\\n    validation_data=val_dataset,\\n    epochs=50,\\n    callbacks=[\\n        tf.keras.callbacks.ModelCheckpoint(\'/curvine-fuse/models/checkpoints/\'),\\n        tf.keras.callbacks.TensorBoard(log_dir=\'/curvine-fuse/logs/\')\\n    ]\\n)\\n```\\n\\n---\\n\\n## \ud83d\uddc4\ufe0f Big Data Ecosystem Integration\\n\\nCurvine seamlessly integrates with mainstream big data frameworks, providing transparent cache acceleration capabilities.\\n\\n### Hadoop Ecosystem Integration\\n\\n#### Basic Configuration\\n\\nAdd in `hdfs-site.xml` or `core-site.xml`:\\n\\n```xml\\n\x3c!-- Curvine FileSystem implementation --\x3e\\n<property>\\n    <name>fs.cv.impl</name>\\n    <value>io.curvine.CurvineFileSystem</value>\\n</property>\\n\\n\x3c!-- Single cluster configuration --\x3e\\n<property>\\n    <name>fs.cv.master_addrs</name>\\n    <value>master1:8995,master2:8995,master3:8995</value>\\n</property>\\n\\n```\\n\\n#### Multi-cluster Support\\n\\n```xml\\n\x3c!-- Cluster 1: Production environment --\x3e\\n<property>\\n    <name>fs.cv.production.master_addrs</name>\\n    <value>prod-master1:8995,prod-master2:8995,prod-master3:8995</value>\\n</property>\\n\\n\x3c!-- Cluster 2: Development environment --\x3e\\n<property>\\n    <name>fs.cv.development.master_addrs</name>\\n    <value>dev-master1:8995,dev-master2:8995</value>\\n</property>\\n\\n\x3c!-- Cluster 3: Machine learning dedicated cluster --\x3e\\n<property>\\n    <name>fs.cv.ml-cluster.master_addrs</name>\\n    <value>ml-master1:8995,ml-master2:8995,ml-master3:8995</value>\\n</property>\\n```\\n\\n\\n### \ud83d\udd04 UFS Transparent Proxy\\n\\nTo better support existing Java applications to seamlessly access Curvine cache, we provide a UFS transparent proxy solution. The core advantage of this solution is **zero code modification**, allowing existing applications to immediately enjoy the cache acceleration effects of Curvine.\\n\\n#### \u2728 Core Features of Transparent Proxy\\n\\n- **\ud83d\udeab Zero code modification**: Preserves all original interfaces unchanged, no business code modifications required\\n- **\ud83d\udd0d Intelligent path recognition**: Only determines whether the path has been mounted to Curvine when opening a file\\n- **\u26a1 Automatic cache acceleration**: Automatically enables cache acceleration for mounted paths, native S3 access for unmounted paths\\n- **\ud83d\udd04 Smooth switching**: Supports dynamically switching whether to use cache at runtime without restarting the application\\n\\n#### \ud83d\udee0\ufe0f Configuration Method\\n\\nSimply replace the S3FileSystem implementation class in Hadoop configuration:\\n\\n```xml\\n\x3c!-- Traditional S3 access configuration --\x3e\\n\x3c!--\\n<property>\\n    <name>fs.s3a.impl</name>\\n    <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>\\n</property>\\n--\x3e\\n\\n\x3c!-- Replace with Curvine transparent proxy --\x3e\\n<property>\\n    <name>fs.s3a.impl</name>\\n    <value>io.curvine.S3AProxyFileSystem</value>\\n</property>\\n\\n<property>\\n    <name>fs.cv.impl</name>\\n    <value>io.curvine.CurvineFileSystem</value>\\n</property>\\n\\n\x3c!-- Curvine cluster configuration --\x3e\\n<property>\\n    <name>fs.curvine.master_addrs</name>\\n    <value>master1:8995,master2:8995,master3:8995</value>\\n</property>\\n```\\n\\n\\n#### \ud83d\udd27 Working Principle\\n\\n![Working Principle](./WorkingPrinciple.png)\\n\\n#### \ud83d\ude80 Usage Example\\n\\n**No need to modify any business code, original code directly enjoys acceleration:**\\n\\n```java\\n// Business code remains completely unchanged!\\nConfiguration conf = new Configuration();\\nFileSystem fs = FileSystem.get(URI.create(\\"s3a://my-bucket/\\"), conf);\\n\\n// If this path is mounted to Curvine, automatically enjoy cache acceleration\\nFSDataInputStream input = fs.open(new Path(\\"s3a://my-bucket/warehouse/data.parquet\\"));\\n\\n// If this path is not mounted, use native S3 access\\nFSDataInputStream input2 = fs.open(new Path(\\"s3a://my-bucket/archive/old-data.parquet\\"));\\n```\\n\\n**Spark/MapReduce code example:**\\n\\n```java\\n// Spark code does not need any modification\\nDataset<Row> df = spark.read()\\n    .option(\\"header\\", \\"true\\")\\n    // If /warehouse/ path is mounted, automatically use cache acceleration\\n    .csv(\\"s3a://data-lake/warehouse/customer_data/\\");\\n    \\ndf.groupBy(\\"region\\")\\n  .agg(sum(\\"revenue\\").alias(\\"total_revenue\\"))\\n  .orderBy(desc(\\"total_revenue\\"))\\n  .show(20);\\n```\\n\\n**Python PySpark example:**\\n\\n```python\\n# Python code also does not need modification\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import sum, desc\\n\\nspark = SparkSession.builder.appName(\\"TransparentCache\\").getOrCreate()\\n\\n# Automatically determine whether to use cache\\ndf = spark.read \\\\\\n    .option(\\"header\\", \\"true\\") \\\\\\n    .csv(\\"s3a://data-lake/analytics/events/\\")\\n\\nresult = df.groupBy(\\"event_type\\") \\\\\\n    .agg(sum(\\"count\\").alias(\\"total_events\\")) \\\\\\n    .orderBy(desc(\\"total_events\\"))\\n    \\nresult.show()\\n```\\n\\n### Apache Spark Optimization Configuration\\n\\n```bash\\n# Spark application startup configuration\\nspark-submit \\\\\\n  --class com.example.SparkApp \\\\\\n  --master yarn \\\\\\n  --deploy-mode cluster \\\\\\n  --conf spark.hadoop.fs.cv.impl=io.curvine.CurvineFileSystem \\\\\\n  --conf spark.hadoop.fs.cv.master_addrs=master1:8995,master2:8995,master3:8995 \\\\\\n  --conf spark.sql.adaptive.enabled=true \\\\\\n  --jars curvine-hadoop-client.jar \\\\\\n  app.jar\\n```\\n\\n#### Spark Code Example\\n\\n```scala\\n// Scala example\\nval spark = SparkSession.builder()\\n  .appName(\\"Curvine Demo\\")\\n  .config(\\"spark.hadoop.fs.cv.impl\\", \\"io.curvine.CurvineFileSystem\\")\\n  .getOrCreate()\\n\\n// Directly use cv:// protocol to access cached data\\nval df = spark.read\\n  .option(\\"multiline\\", \\"true\\")\\n  .json(\\"cv://production/warehouse/events/2024/01/01/\\")\\n\\ndf.groupBy(\\"event_type\\")\\n  .count()\\n  .show()\\n\\n// Multi-cluster access\\nval prodData = spark.read.parquet(\\"cv://production/warehouse/sales/\\")\\nval mlData = spark.read.parquet(\\"cv://ml-cluster/features/user_profiles/\\")\\n```\\n\\n```python\\n# Python example\\nfrom pyspark.sql import SparkSession\\n\\nspark = SparkSession.builder \\\\\\n    .appName(\\"Curvine Python Demo\\") \\\\\\n    .config(\\"spark.hadoop.fs.cv.impl\\", \\"io.curvine.CurvineFileSystem\\") \\\\\\n    .config(\\"spark.hadoop.fs.cv.master_addrs\\", \\"master1:8995,master2:8995\\") \\\\\\n    .getOrCreate()\\n\\n# Read data from cache\\ndf = spark.read.option(\\"header\\", \\"true\\") \\\\\\n    .csv(\\"cv://warehouse/customer_data/\\")\\n\\n# Complex queries automatically enjoy cache acceleration\\nresult = df.groupBy(\\"region\\") \\\\\\n    .agg({\\"revenue\\": \\"sum\\", \\"orders\\": \\"count\\"}) \\\\\\n    .orderBy(\\"sum(revenue)\\", ascending=False)\\n\\nresult.show(20)\\n```\\n\\n### Trino/Presto Plugin Integration\\n\\nCurvine provides an intelligent path replacement plugin, which can achieve non-invasive cache acceleration without requiring business code modifications, achieving completely transparent cache acceleration:\\n\\n#### Plugin Workflow\\n\\n![Plugin Workflow](./PluginWorkflow.png)\\n\\nSpark plugin usage example:\\n```\\nspark-submit \\\\\\n--class main.scala.Tpch \\\\\\n--name tpch_demo \\\\\\n--conf spark.hadoop.fs.cv.impl=io.curvine.CurvineFileSystem \\\\\\n--conf spark.hadoop.fs.cv.default.master_addrs=master1:8995,master2:8995 \\\\\\n--conf spark.sql.extensions=io.curvine.spark.CurvineSparkExtension \\\\\\n```\\n\\n### Flink Real-time Computing Integration\\n\\n```java\\n// Flink Table API integration example\\nTableEnvironment tableEnv = TableEnvironment.create(settings);\\n\\n// Configure Curvine FileSystem\\nConfiguration config = new Configuration();\\nconfig.setString(\\"fs.cv.impl\\", \\"io.curvine.CurvineFileSystem\\");\\nconfig.setString(\\"fs.cv.master_addrs\\", \\"master1:8995,master2:8995\\");\\n\\n// Create Curvine table\\ntableEnv.executeSql(\\n    \\"CREATE TABLE user_events (\\" +\\n    \\"  user_id BIGINT,\\" +\\n    \\"  event_type STRING,\\" +\\n    \\"  timestamp_ms BIGINT,\\" +\\n    \\"  properties MAP<STRING, STRING>\\" +\\n    \\") WITH (\\" +\\n    \\"  \'connector\' = \'filesystem\',\\" +\\n    \\"  \'path\' = \'cv://streaming/events/\',\\" +\\n    \\"  \'format\' = \'json\'\\" +\\n    \\")\\"\\n);\\n\\n// Real-time query enjoys cache acceleration\\nTable result = tableEnv.sqlQuery(\\n    \\"SELECT user_id, COUNT(*) as event_count \\" +\\n    \\"FROM user_events \\" +\\n    \\"WHERE timestamp_ms > UNIX_TIMESTAMP() * 1000 - 3600000 \\" +\\n    \\"GROUP BY user_id\\"\\n);\\n```\\n\\n---\\n\\n\\n---\\n\\n## \ud83d\udca1 Best Practices\\n\\n### \ud83c\udfaf Mounting Strategy Best Practices\\n\\n#### Tiered Mounting by Business Scenarios\\n\\n```bash\\n# Hot data: high-frequency access, using memory cache\\nbin/cv mount s3://bucket/hot /bucket/hot \\\\\\n  --storage-type=mem \\\\\\n  --replicas=3 \\\\\\n  --ttl-ms=1d \\\\\\n  --ttl-action=delete\\n\\n# Warm data: regular access, using SSD cache\\nbin/cv mount s3://bucket/warm /bucket/warm \\\\\\n  --storage-type=ssd \\\\\\n  --replicas=2 \\\\\\n  --ttl-ms=7d \\\\\\n  --ttl-action=delete\\n\\n\\n# Cold data: low-frequency access, using disk cache\\nbin/cv mount s3://bucket/cold /bucket/cold \\\\\\n  --storage-type=disk \\\\\\n  --replicas=1 \\\\\\n  --ttl-ms=30d \\\\\\n  --ttl-action=delete\\n```\\n\\n#### Optimization by Data Type\\n\\n```bash\\n# Small file intensive (e.g., logs, configurations)\\nbin/cv mount s3://bucket/logs /bucket/logs \\\\\\n  --block-size=4MB \\\\\\n  --consistency-strategy=none \\n\\n# Large file type (e.g., videos, models)\\nbin/cv mount s3://bucket/models /bucket/models \\\\\\n  --block-size=1GB \\\\\\n  --consistency-strategy=always \\n\\n# Analytical data (e.g., Parquet)\\nbin/cv mount s3://bucket/analytics /bucket/analytics \\\\\\n  --block-size=128MB \\\\\\n  --consistency-strategy=none \\\\\\n```\\n---\\n\\n## \ud83c\udfaf Summary\\n\\nAs a new generation distributed caching system, Curvine provides excellent performance improvements for modern data-intensive applications through intelligent caching strategies, strong consistency guarantees, and seamless ecosystem integration.\\n\\n### \ud83c\udfc6 Core Values\\n\\n- **\ud83d\ude80 Performance Improvement**: 10-100x access acceleration, significantly reducing data access latency\\n- **\ud83d\udcb0 Cost Optimization**: Reduce cloud storage access costs, improve computing resource utilization  \\n- **\ud83d\udee1\ufe0f Data Security**: Multiple consistency guarantees to ensure data accuracy and integrity\\n- **\ud83c\udf10 Ecosystem Friendly**: Seamless integration with mainstream big data and AI frameworks\\n\\n---\\n\\n*Curvine - Make data access lightning fast \u26a1*"},{"id":"/2025/08/08/fio-bench","metadata":{"permalink":"/blog/2025/08/08/fio-bench","editUrl":"https://github.com/curvineio/curvine-doc/edit/main/blog/blog/2025-08-08-fio-bench/index.md","source":"@site/blog/2025-08-08-fio-bench/index.md","title":"Building a Curvine Cluster from Scratch & FIO Testing","description":"How to quickly get started and try out Curvine\'s performance? This article will introduce how to build a local small cluster from scratch, allowing everyone to get hands-on experience quickly.","date":"2025-08-08T00:00:00.000Z","tags":[{"inline":false,"label":"benchmark","permalink":"/blog/tags/benchmark","description":"benchmark of curvine"}],"readingTime":1.73,"hasTruncateMarker":false,"authors":[{"name":"David","title":"Founder of Curvine","url":"https://curvine.io","page":{"permalink":"/blog/authors/all-sebastien-lorber-articles"},"socials":{"github":"https://github.com/szbr9486","x":"https://x.com/szbr8896"},"key":"david"}],"frontMatter":{"authors":["david"],"tags":["benchmark"]},"unlisted":false,"prevItem":{"title":"Curvine Distributed Cache System User Guide","permalink":"/blog/2025/09/28/user-guide"},"nextItem":{"title":"Curvine: High-Performance Distributed Cache\u2014\u2014DataFun Intelligent Conference","permalink":"/blog/2025/07/25/datafun-conference"}},"content":"How to quickly get started and try out Curvine\'s performance? This article will introduce how to build a local small cluster from scratch, allowing everyone to get hands-on experience quickly.\\n\\n> GitHub: https://github.com/CurvineIO/curvine\\n\\n----\\n\\n## 1. Download the Code:\\n```bash\\ngit clone https://github.com/CurvineIO/curvine.git\\n```\\n\\n## 2. Environment Requirements:\\n```bash\\nGCC: version 10 or later \\nRust: version 1.86 or later \\nProtobuf: version 3.x\\nMaven: version 3.8 or later\\nLLVM: version 12 or later\\nFUSE: libfuse2 or libfuse3 development packages\\nJDK: version 1.8 or later \\nnpm: version 9 or later\\nPython: version 3.7 or later \\n```\\n\\n## 3. Compile & Run\\n```bash\\nmake all\\n```\\n\\nTo facilitate compilation, our build script will check dependencies in advance. For macOS users, we will temporarily skip FUSE compilation (currently not adapted for macOS). Interested users can consider using the `macfuse` project for adaptation.\\n\\n![make-checkenv](./make-checkenv.png)\\n\\n## 4. After Compilation, Start Local Cluster\\n```bash\\ncd build/dist\\n./bin/restart-all.sh\\n```\\n\\nAfter successful startup, execute the report command to check if it\'s working:\\n```bash\\n\\nbin/cv report\\n\\n       active_master: localhost:8995\\n       journal_nodes: 1,localhost:8996\\n            capacity: 233.5GB\\n           available: 105.0GB (44.99%)\\n             fs_used: 0.0B (0.00%)\\n         non_fs_used: 128.4GB\\n     live_worker_num: 1\\n     lost_worker_num: 0\\n           inode_num: 0\\n           block_num: 0\\n    live_worker_list: 192.168.xxx.xxx:8997,105.0GB/233.5GB (44.99%)\\n    lost_worker_list:\\n```\\n\\n## 5. View Local Master and Worker WebUI\\n\\n```bash\\nhttp://localhost:9000/\\nhttp://localhost:9001/\\n```\\n\\n![webui](./webui.png)\\n\\n## 6. FIO Testing\\nTest Environment: Alibaba Cloud `ecs.r8a.8xlarge` instance with one master/worker/client each\\n- 32 cores (vCPU)\\n- 256 GiB memory  \\n- System disk and data disk both: ESSD cloud disk 500 GiB (7800 IOPS)\\n- Maximum bandwidth: 25Gb\\n\\nPrepare data (on worker machine):\\n\\n```bash\\nbin/curvine-bench.sh fuse.write\\n```\\n\\n**FIO Sequential Read Test, 8 Concurrent Jobs**\\n\\n```bash\\nfio -iodepth=1 -rw=read -ioengine=libaio -bs=256k\\n -group_reporting -size=200gb \\n -filename=/curvine-fuse/fs-bench/0  \\n -name=read_test --readonly -direct=1 --runtime=60 \\n -numjobs=8\\n```\\n\\n**FIO Random Read Test, 8 Concurrent Jobs**\\n```bash\\n\\nfio -iodepth=1 -rw=randread -ioengine=libaio -bs=256k\\n -group_reporting -size=200gb \\n -filename=/curvine-fuse/fs-bench/0  \\n -name=read_test --readonly -direct=1 --runtime=60\\n -numjobs=8\\n```\\n\\nFinally, here\'s a video demonstration of the FIO testing results:"},{"id":"/2025/07/25/datafun-conference","metadata":{"permalink":"/blog/2025/07/25/datafun-conference","editUrl":"https://github.com/curvineio/curvine-doc/edit/main/blog/blog/2025-07-25-datafun-conference/index.md","source":"@site/blog/2025-07-25-datafun-conference/index.md","title":"Curvine: High-Performance Distributed Cache\u2014\u2014DataFun Intelligent Conference","description":"","date":"2025-07-25T00:00:00.000Z","tags":[{"inline":false,"label":"teams","permalink":"/blog/tags/teams","description":"teams activities of curvine"}],"readingTime":1.9,"hasTruncateMarker":false,"authors":[{"name":"David","title":"Founder of Curvine","url":"https://curvine.io","page":{"permalink":"/blog/authors/all-sebastien-lorber-articles"},"socials":{"github":"https://github.com/szbr9486","x":"https://x.com/szbr8896"},"key":"david"}],"frontMatter":{"authors":["david"],"tags":["teams"]},"unlisted":false,"prevItem":{"title":"Building a Curvine Cluster from Scratch & FIO Testing","permalink":"/blog/2025/08/08/fio-bench"},"nextItem":{"title":"what-is-curvine","permalink":"/blog/2025/07/15/what-is-curvine"}},"content":"<div style={{ textAlign: \'center\' }}>\\n  <img src={require(\\"./page1.png\\").default} alt=\\"page1\\" style={{ width: \'100%\', maxWidth: \'800px\' }}></img>\\n</div>\\n<div style={{ textAlign: \'center\' }}>\\n  <img src={require(\\"./page2.png\\").default} alt=\\"page2\\" style={{ width: \'100%\', maxWidth: \'800px\' }}></img>\\n</div>\\n<div style={{ textAlign: \'center\' }}>\\n  <img src={require(\\"./page3.png\\").default} alt=\\"page3\\" style={{ width: \'100%\', maxWidth: \'800px\' }}></img>\\n</div>\\n<div style={{ textAlign: \'center\' }}>\\n  <img src={require(\\"./page4.png\\").default} alt=\\"page4\\" style={{ width: \'100%\', maxWidth: \'800px\' }}></img>\\n</div>\\n<div style={{ textAlign: \'center\' }}>\\n  <img src={require(\\"./page5.png\\").default} alt=\\"page5\\" style={{ width: \'100%\', maxWidth: \'800px\' }}></img>\\n</div>\\n<div style={{ textAlign: \'center\' }}>\\n  <img src={require(\\"./page6.png\\").default} alt=\\"page6\\" style={{ width: \'100%\', maxWidth: \'800px\' }}></img>\\n</div>\\n<div style={{ textAlign: \'center\' }}>\\n  <img src={require(\\"./page7.png\\").default} alt=\\"page7\\" style={{ width: \'100%\', maxWidth: \'800px\' }}></img>\\n</div>\\n<div style={{ textAlign: \'center\' }}>\\n  <img src={require(\\"./page8.png\\").default} alt=\\"page8\\" style={{ width: \'100%\', maxWidth: \'800px\' }}></img>\\n</div>\\n<div style={{ textAlign: \'center\' }}>\\n  <img src={require(\\"./page9.png\\").default} alt=\\"page9\\" style={{ width: \'100%\', maxWidth: \'800px\' }}></img>\\n</div>\\n<div style={{ textAlign: \'center\' }}>\\n  <img src={require(\\"./page10.png\\").default} alt=\\"page10\\" style={{ width: \'100%\', maxWidth: \'800px\' }}></img>\\n</div>\\n<div style={{ textAlign: \'center\' }}>\\n  <img src={require(\\"./page11.png\\").default} alt=\\"page11\\" style={{ width: \'100%\', maxWidth: \'800px\' }}></img>\\n</div>\\n<div style={{ textAlign: \'center\' }}>\\n  <img src={require(\\"./page12.png\\").default} alt=\\"page12\\" style={{ width: \'100%\', maxWidth: \'800px\' }}></img>\\n</div>\\n<div style={{ textAlign: \'center\' }}>\\n  <img src={require(\\"./page13.png\\").default} alt=\\"page13\\" style={{ width: \'100%\', maxWidth: \'800px\' }}></img>\\n</div>\\n<div style={{ textAlign: \'center\' }}>\\n  <img src={require(\\"./page14.png\\").default} alt=\\"page14\\" style={{ width: \'100%\', maxWidth: \'800px\' }}></img>\\n</div>\\n<div style={{ textAlign: \'center\' }}>\\n  <img src={require(\\"./page15.png\\").default} alt=\\"page15\\" style={{ width: \'100%\', maxWidth: \'800px\' }}></img>\\n</div>\\n<div style={{ textAlign: \'center\' }}>\\n  <img src={require(\\"./page16.png\\").default} alt=\\"page16\\" style={{ width: \'100%\', maxWidth: \'800px\' }}></img>\\n</div>\\n<div style={{ textAlign: \'center\' }}>\\n  <img src={require(\\"./page17.png\\").default} alt=\\"page17\\" style={{ width: \'100%\', maxWidth: \'800px\' }}></img>\\n</div>\\n<div style={{ textAlign: \'center\' }}>\\n  <img src={require(\\"./page18.png\\").default} alt=\\"page18\\" style={{ width: \'100%\', maxWidth: \'800px\' }}></img>\\n</div>\\n<div style={{ textAlign: \'center\' }}>\\n  <img src={require(\\"./page19.png\\").default} alt=\\"page19\\" style={{ width: \'100%\', maxWidth: \'800px\' }}></img>\\n</div>\\n<div style={{ textAlign: \'center\' }}>\\n  <img src={require(\\"./page20.png\\").default} alt=\\"page20\\" style={{ width: \'100%\', maxWidth: \'800px\' }}></img>\\n</div>"},{"id":"/2025/07/15/what-is-curvine","metadata":{"permalink":"/blog/2025/07/15/what-is-curvine","editUrl":"https://github.com/curvineio/curvine-doc/edit/main/blog/blog/2025-07-15-what-is-curvine/index.md","source":"@site/blog/2025-07-15-what-is-curvine/index.md","title":"what-is-curvine","description":"What is Curvine","date":"2025-07-15T00:00:00.000Z","tags":[{"inline":false,"label":"teams","permalink":"/blog/tags/teams","description":"teams activities of curvine"}],"readingTime":5.79,"hasTruncateMarker":true,"authors":[{"name":"David","title":"Founder of Curvine","url":"https://curvine.io","page":{"permalink":"/blog/authors/all-sebastien-lorber-articles"},"socials":{"github":"https://github.com/szbr9486","x":"https://x.com/szbr8896"},"key":"david"}],"frontMatter":{"authors":["david"],"tags":["teams"]},"unlisted":false,"prevItem":{"title":"Curvine: High-Performance Distributed Cache\u2014\u2014DataFun Intelligent Conference","permalink":"/blog/2025/07/25/datafun-conference"},"nextItem":{"title":"Curvine Caching now comming!","permalink":"/blog/welcome"}},"content":"\x3c!-- truncate --\x3e\\n\\n# Curvine: High-Performance Distributed Cache(Now Open Source)\\n\\n## What is Curvine\\n\\n&emsp;Curvine is a distributed caching system implemented in Rust, featuring high concurrency, high throughput, low latency, and low resource consumption. Unlike KV caches like Redis or TiKV, Curvine exclusively provides file caching capabilities. It is not a storage system but rather a caching layer - data persistence still relies on underlying file systems or object storage systems for support.\\n\\n## What problem does it solve\\n\\n1. Large-scale Data I/O Performance Bottlenecks;\\n2. Single-Machine Cache Capacity Limitations.\\n\\n&emsp;In practical applications, what scenarios are suitable for Curvine acceleration?\\n\\n<div style={{ textAlign: \'center\' }}>\\n  <img src={require(\\"./Curvine-application.png\\").default} alt=\\"Curvine Application Scenarios.\\" style={{ width: \'80%\', maxWidth: \'800px\' }}></img>\\n  <p style={{ fontSize: \'0.8em\', color: \'#666\', marginTop: \'8px\' }}>\\n    <b>Fig. 1</b>\uff1aCurvine Application Scenarios.\\n  </p>\\n</div>\\n\\n&emsp;As shown in the figure above, Curvine is designed for the following five core scenarios:\\n\\n1. Accelerating intermediate data processing in big data shuffle operations  \\n2. Caching hot table data for faster big data analytics  \\n3. Boosting AI training efficiency through dataset caching  \\n4. Accelerating model file distribution via caching layer  \\n5. Cross-cloud data caching to mitigate performance bottlenecks of dedicated cloud connections  \\n\\n&emsp;These use cases are just the beginning. In simple terms, Curvine fundamentally addresses: The growing conflict between escalating computational demands and the I/O bottlenecks of distributed cache systems.\\n\\n## Performance\\n&emsp;We demonstrate performance and resource utilization from the following aspects:\u200b\\n\\n**1. Metadata operation performance**\\n\x3c!-- \u8868\u683c\u533a --\x3e\\n   <table>\\n  <thead>\\n    <tr style={{ backgroundColor: \'#2ecc71\', color: \'white\' }}>\\n      <th>Operation Type</th>\\n      <th>Curvine (QPS)</th>\\n      <th>Juicefs (QPS)</th>\\n      <th>oss (QPS)</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <td>create</td>\\n      <td style={{ textAlign: \'right\' }}>19,985</td>\\n      <td style={{ textAlign: \'right\' }}>16,000</td>\\n      <td style={{ textAlign: \'right\' }}>2,000</td>\\n    </tr>\\n    <tr>\\n      <td>open</td>\\n      <td style={{ textAlign: \'right\' }}>60,376</td>\\n      <td style={{ textAlign: \'right\' }}>50,000</td>\\n      <td style={{ textAlign: \'right\' }}>3,900</td>\\n    </tr>\\n    <tr>\\n      <td>rename</td>\\n      <td style={{ textAlign: \'right\' }}>43,009</td>\\n      <td style={{ textAlign: \'right\' }}>21,000</td>\\n      <td style={{ textAlign: \'right\' }}>200</td>\\n    </tr>\\n    <tr>\\n      <td>delete</td>\\n      <td style={{ textAlign: \'right\' }}>39,013</td>\\n      <td style={{ textAlign: \'right\' }}>41,000</td>\\n      <td style={{ textAlign: \'right\' }}>1,900</td>\\n    </tr>\\n  </tbody>\\n</table>\\n\\n&emsp;**Note**: All benchmark comparisons were conducted with a concurrency level of 40.\\n\\n&emsp;**Detailed results**: https://curvineio.github.io/docs/Benchmark/meta/\\n\\n&emsp;**Industry benchmark test data of comparable products**: https://juicefs.com/zh-cn/blog/engineering/meta-perf-hdfs-oss-jfs\\n\\n\\n**2. Data Read/Write Performance**\\n\\n&emsp;Benchmarking Alluxio performance under identical hardware conditions:\\n\\n\u25cf 256K sequential read\\n  \x3c!-- \u8868\u683c\u533a --\x3e\\n  <table style={{ width: \'100%\', borderCollapse: \'collapse\' }}>\\n    <thead>\\n      <tr style={{ backgroundColor: \'#2ecc71\', color: \'white\' }}>\\n        <th>Thread count</th>\\n        <th>Curvine Open Source Edition (GiB/s)</th>\\n        <th>Throughput of Open Source Alluxio (GiB/s)</th>\\n      </tr>\\n    </thead>\\n    <tbody>\\n      \x3c!-- \u6570\u636e\u884c\u6a21\u677f --\x3e\\n      <tr style={{ borderBottom: \'1px solid #e1e4e8\' }}>\\n        <td style={{ textAlign: \'right\' }}>1</td>\\n        <td style={{ textAlign: \'right\' }}>2.2</td>\\n        <td style={{ textAlign: \'right\' }}>0.6</td>\\n      </tr>\\n      <tr style={{ borderBottom: \'1px solid #e1e4e8\' }}>\\n        <td style={{ textAlign: \'right\' }}>2</td>\\n        <td style={{ textAlign: \'right\' }}>3.7</td>\\n        <td style={{ textAlign: \'right\' }}>1.1</td>\\n      </tr>\\n      <tr style={{ borderBottom: \'1px solid #e1e4e8\' }}>\\n        <td style={{ textAlign: \'right\' }}>4</td>\\n        <td style={{ textAlign: \'right\' }}>6.8</td>\\n        <td style={{ textAlign: \'right\' }}>2.3</td>\\n      </tr>\\n      <tr style={{ borderBottom: \'1px solid #e1e4e8\' }}>\\n        <td style={{ textAlign: \'right\' }}>8</td>\\n        <td style={{ textAlign: \'right\' }}>8.9</td>\\n        <td style={{ textAlign: \'right\' }}>4.5</td>\\n      </tr>\\n      <tr style={{ borderBottom: \'1px solid #e1e4e8\' }}>\\n        <td style={{ textAlign: \'right\' }}>16</td>\\n        <td style={{ textAlign: \'right\' }}>9.2</td>\\n        <td style={{ textAlign: \'right\' }}>7.9</td>\\n      </tr>\\n      <tr style={{ borderBottom: \'1px solid #e1e4e8\' }}>\\n        <td style={{ textAlign: \'right\' }}>32</td>\\n        <td style={{ textAlign: \'right\' }}>9.5</td>\\n        <td style={{ textAlign: \'right\' }}>8.8</td>\\n      </tr>\\n      <tr style={{ borderBottom: \'1px solid #e1e4e8\' }}>\\n        <td style={{ textAlign: \'right\' }}>64</td>\\n        <td style={{ textAlign: \'right\' }}>9.2</td>\\n        <td style={{ textAlign: \'right\' }}>N/A</td>\\n      </tr>\\n      <tr style={{ borderBottom: \'1px solid #e1e4e8\' }}>\\n        <td style={{ textAlign: \'right\' }}>128</td>\\n        <td style={{ textAlign: \'right\' }}>9.2</td>\\n        <td style={{ textAlign: \'right\' }}>N/A</td>\\n      </tr>\\n    </tbody>\\n  </table>\\n  \\n\u25cf 256K random read\\n\x3c!-- \u8868\u683c\u533a --\x3e\\n  <table style={{ width: \'100%\', borderCollapse: \'collapse\' }}>\\n    <thead>\\n      <tr style={{ backgroundColor: \'#2ecc71\', color: \'white\' }}>\\n        <th>Thread count</th>\\n        <th>Curvine Open Source Edition (GiB/s)</th>\\n        <th>Throughput of Open Source Alluxio (GiB/s)</th>\\n      </tr>\\n    </thead>\\n    <tbody>\\n      \x3c!-- \u6570\u636e\u884c\u6a21\u677f --\x3e\\n      <tr style={{ borderBottom: \'1px solid #e1e4e8\' }}>\\n        <td style={{ textAlign: \'right\' }}>1</td>\\n        <td style={{ textAlign: \'right\' }}>0.3</td>\\n        <td style={{ textAlign: \'right\' }}>0.0</td>\\n      </tr>\\n      <tr style={{ borderBottom: \'1px solid #e1e4e8\' }}>\\n        <td style={{ textAlign: \'right\' }}>2</td>\\n        <td style={{ textAlign: \'right\' }}>0.7</td>\\n        <td style={{ textAlign: \'right\' }}>0.1</td>\\n      </tr>\\n      <tr style={{ borderBottom: \'1px solid #e1e4e8\' }}>\\n        <td style={{ textAlign: \'right\' }}>4</td>\\n        <td style={{ textAlign: \'right\' }}>1.4</td>\\n        <td style={{ textAlign: \'right\' }}>0.1</td>\\n      </tr>\\n      <tr style={{ borderBottom: \'1px solid #e1e4e8\' }}>\\n        <td style={{ textAlign: \'right\' }}>8</td>\\n        <td style={{ textAlign: \'right\' }}>2.8</td>\\n        <td style={{ textAlign: \'right\' }}>0.2</td>\\n      </tr>\\n      <tr style={{ borderBottom: \'1px solid #e1e4e8\' }}>\\n        <td style={{ textAlign: \'right\' }}>16</td>\\n        <td style={{ textAlign: \'right\' }}>5.2</td>\\n        <td style={{ textAlign: \'right\' }}>0.4</td>\\n      </tr>\\n      <tr style={{ borderBottom: \'1px solid #e1e4e8\' }}>\\n        <td style={{ textAlign: \'right\' }}>32</td>\\n        <td style={{ textAlign: \'right\' }}>7.8</td>\\n        <td style={{ textAlign: \'right\' }}>0.3</td>\\n      </tr>\\n      <tr style={{ borderBottom: \'1px solid #e1e4e8\' }}>\\n        <td style={{ textAlign: \'right\' }}>64</td>\\n        <td style={{ textAlign: \'right\' }}>8.7</td>\\n        <td style={{ textAlign: \'right\' }}>N/A</td>\\n      </tr>\\n      <tr style={{ borderBottom: \'1px solid #e1e4e8\' }}>\\n        <td style={{ textAlign: \'right\' }}>128</td>\\n        <td style={{ textAlign: \'right\' }}>9.0</td>\\n        <td style={{ textAlign: \'right\' }}>N/A</td>\\n      </tr>\\n    </tbody>\\n  </table>\\n\\n &emsp; Data disclosure from Alluxio official website: https://www.alluxio.com.cn/alluxio-enterprise-vs-open-source/.\\n\\n**3. Resource consumption**\\n\\n\\n &emsp; Thanks to Rust\'s language features, in the big data shuffle acceleration scenario, our comparison of online resource consumption between Curvine and Alluxio shows a \u200b90%+ reduction in memory usage\u200b and \u200b50%+ reduction in CPU usage.\\n\\n ## Architecture Overview\\n &emsp; Curvine\'s architectural design philosophy: Simplicity, Excellence, and Universality.\\n\\n<div style={{ textAlign: \'center\' }}>\\n  <img src={require(\\"./Curvine-architechure.png\\").default} alt=\\"Curvine Architecture Diagram.\\" style={{ width: \'80%\', maxWidth: \'800px\' }}></img>\\n  <p style={{ fontSize: \'0.8em\', color: \'#666\', marginTop: \'8px\' }}>\\n    <b>Fig. 2</b>\uff1aCurvine Application Scenarios.\\n  </p>\\n</div>\\n\\n&emsp;**Simplicity**: Lightweight design with only two roles in the caching service: master and worker. For non-performance-critical modules, maximize reuse of open-source or existing technologies, ensuring minimal code complexity.\\n\\n&emsp;**Excellence**: Key performance-impacting components (e.g., underlying RPC communication framework, Fuse implementation) are independently designed and optimized with a performance-first mindset.\\n\\n&emsp;**Generality**: Compatible with multiple existing access modes. The underlying storage supports mainstream distributed file and object storage systems, ensuring versatility and ease of use.\\n\\n## On Open-Source\\n&emsp;We have achieved significant performance gains by deploying Curvine in high-concurrency, high-throughput big data scenarios internally. Now, we aim to collaborate with external partners to co-build this solution and collectively accelerate the infrastructure transition to Rust.\\n\\n&emsp;https://github.com/curvineio/curvine\\n\\n&emsp;Powered by OPPO Bigdata."},{"id":"welcome","metadata":{"permalink":"/blog/welcome","editUrl":"https://github.com/curvineio/curvine-doc/edit/main/blog/blog/2025-05-29-welcome/index.md","source":"@site/blog/2025-05-29-welcome/index.md","title":"Curvine Caching now comming!","description":"Curvine is a high-performance distributed caching system implemented in Rust, designed for low-latency and high-throughput workloads with powerful data governance capabilities.","date":"2025-05-29T00:00:00.000Z","tags":[{"inline":false,"label":"teams","permalink":"/blog/tags/teams","description":"teams activities of curvine"}],"readingTime":0.13,"hasTruncateMarker":true,"authors":[{"name":"Barry","title":"Senior Engineer","url":"https://curvine.io","page":{"permalink":"/blog/authors/barry"},"socials":{"github":"https://github.com/lzjqsdd","x":"https://x.com/infmaxtop"},"imageURL":"https://infmax.top/img/author.png","key":"barry"},{"name":"David","title":"Founder of Curvine","url":"https://curvine.io","page":{"permalink":"/blog/authors/all-sebastien-lorber-articles"},"socials":{"github":"https://github.com/szbr9486","x":"https://x.com/szbr8896"},"key":"david"}],"frontMatter":{"slug":"welcome","title":"Curvine Caching now comming!","authors":["barry","david"],"tags":["teams"]},"unlisted":false,"prevItem":{"title":"what-is-curvine","permalink":"/blog/2025/07/15/what-is-curvine"}},"content":"\x3c!-- truncate --\x3e\\n\\nCurvine is a high-performance distributed caching system implemented in Rust, designed for low-latency and high-throughput workloads with powerful data governance capabilities."}]}}')}}]);