"use strict";(self.webpackChunkcurvine_doc=self.webpackChunkcurvine_doc||[]).push([[299],{8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>a});var r=s(6540);const t={},i=r.createContext(t);function o(e){const n=r.useContext(i);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),r.createElement(i.Provider,{value:n},e.children)}},8917:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"Architecture/opendal-integration","title":"OpenDAL Ecosystem Integration","description":"Overview","source":"@site/docs/5-Architecture/02-opendal-integration.md","sourceDirName":"5-Architecture","slug":"/Architecture/opendal-integration","permalink":"/docs/Architecture/opendal-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/curvineio/curvine-doc/edit/main/docs/5-Architecture/02-opendal-integration.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Architecture Overview","permalink":"/docs/Architecture/instrodction"},"next":{"title":"Fluid Integration","permalink":"/docs/Architecture/fluid-integration"}}');var t=s(4848),i=s(8453);const o={},a="OpenDAL Ecosystem Integration",d={},c=[{value:"Overview",id:"overview",level:2},{value:"What is OpenDAL?",id:"what-is-opendal",level:2},{value:"Supported Storage Backends",id:"supported-storage-backends",level:2},{value:"Object Storage",id:"object-storage",level:3},{value:"Distributed File Systems",id:"distributed-file-systems",level:3},{value:"Core Features",id:"core-features",level:2},{value:"Mount Feature - Mounting External Storage",id:"mount-feature---mounting-external-storage",level:3},{value:"S3 Storage Mount Examples",id:"s3-storage-mount-examples",level:4},{value:"Other Storage Type Mounts",id:"other-storage-type-mounts",level:4},{value:"Load Feature - Loading External Data",id:"load-feature---loading-external-data",level:3},{value:"Single File Load Examples",id:"single-file-load-examples",level:4},{value:"Batch Load Examples",id:"batch-load-examples",level:4},{value:"Configuration Parameters",id:"configuration-parameters",level:3},{value:"S3-Compatible Storage Configuration",id:"s3-compatible-storage-configuration",level:4},{value:"OSS Configuration",id:"oss-configuration",level:4},{value:"HDFS Configuration",id:"hdfs-configuration",level:4},{value:"HDFS Environment Requirements",id:"hdfs-environment-requirements",level:3},{value:"Compilation Requirements",id:"compilation-requirements",level:4},{value:"Runtime Environment Requirements",id:"runtime-environment-requirements",level:4}];function l(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"opendal-ecosystem-integration",children:"OpenDAL Ecosystem Integration"})}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsxs)(n.p,{children:["Curvine integrates with ",(0,t.jsx)(n.a,{href:"https://opendal.apache.org/",children:"OpenDAL"})," (Open Data Access Layer) at its core, providing powerful storage ecosystem support. This integration enables Curvine to work seamlessly with various storage backends while maintaining a unified interface."]}),"\n",(0,t.jsx)(n.h2,{id:"what-is-opendal",children:"What is OpenDAL?"}),"\n",(0,t.jsx)(n.p,{children:"OpenDAL is a data access layer that provides unified APIs to access various storage services. It serves as an abstraction layer between applications and storage systems, offering:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Unified Interface"}),": Consistent APIs across different storage backends"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-Storage Support"}),": Support for object storage, filesystems, databases, and more"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"High Performance"}),": Optimized data access paths and caching mechanisms"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Extensibility"}),": Easy addition of new storage backend support"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Reliability"}),": Built-in retry mechanisms and error handling"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"supported-storage-backends",children:"Supported Storage Backends"}),"\n",(0,t.jsx)(n.p,{children:"Through OpenDAL integration, Curvine currently supports the following storage services:"}),"\n",(0,t.jsx)(n.h3,{id:"object-storage",children:"Object Storage"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Amazon S3"}),": AWS object storage service, supports ",(0,t.jsx)(n.code,{children:"s3://"})," and ",(0,t.jsx)(n.code,{children:"s3a://"})," protocols"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Alibaba Cloud OSS"}),": Alibaba Cloud object storage service, supports ",(0,t.jsx)(n.code,{children:"oss://"})," protocol"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Tencent Cloud COS"}),": Tencent Cloud object storage, supports ",(0,t.jsx)(n.code,{children:"cos://"})," protocol"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Azure Blob Storage"}),": Microsoft Azure object storage, supports ",(0,t.jsx)(n.code,{children:"azblob://"})," protocol"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Google Cloud Storage"}),": Google cloud storage, supports ",(0,t.jsx)(n.code,{children:"gcs://"})," and ",(0,t.jsx)(n.code,{children:"gs://"})," protocols"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"distributed-file-systems",children:"Distributed File Systems"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"HDFS"}),": Hadoop Distributed File System, supports ",(0,t.jsx)(n.code,{children:"hdfs://"})," protocol (requires special environment setup)"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"core-features",children:"Core Features"}),"\n",(0,t.jsx)(n.p,{children:"Through OpenDAL integration, Curvine provides two core features for accessing external storage:"}),"\n",(0,t.jsx)(n.h3,{id:"mount-feature---mounting-external-storage",children:"Mount Feature - Mounting External Storage"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"mount"})," feature allows mounting external storage systems into Curvine's namespace, making external storage appear as part of the Curvine filesystem."]}),"\n",(0,t.jsx)(n.h4,{id:"s3-storage-mount-examples",children:"S3 Storage Mount Examples"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Mount S3 bucket to local path\n./dist/bin/cv mount s3://flink/user /mnt/s3 \\\n    -c s3.endpoint_url=http://s3v2.dg-access-test.wanyol.com \\\n    -c s3.region_name=cn-south-1 \\\n    -c s3.credentials.access=*** \\\n    -c s3.credentials.secret=***\n"})}),"\n",(0,t.jsx)(n.h4,{id:"other-storage-type-mounts",children:"Other Storage Type Mounts"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Mount Alibaba Cloud OSS\n./dist/bin/cv mount oss://my-bucket/data /mnt/oss \\\n    -c oss.endpoint_url=https://oss-cn-hangzhou.aliyuncs.com \\\n    -c oss.credentials.access_key_id=*** \\\n    -c oss.credentials.access_key_secret=***\n\n# Mount HDFS (requires special environment setup)\n./dist/bin/cv mount hdfs://namenode:9000/data /mnt/hdfs\n"})}),"\n",(0,t.jsx)(n.h3,{id:"load-feature---loading-external-data",children:"Load Feature - Loading External Data"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"load"})," feature allows directly loading data from external storage into Curvine, supporting single file or batch loading."]}),"\n",(0,t.jsx)(n.h4,{id:"single-file-load-examples",children:"Single File Load Examples"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Load single file from S3\n./dist/bin/cv load s3://flink/user/simple_test.txt \\\n    -c s3.endpoint_url=http://s3v2.dg-access-test.wanyol.com \\\n    -c s3.region_name=cn-south-1 \\\n    -c s3.credentials.access=*** \\\n    -c s3.credentials.secret=***\n"})}),"\n",(0,t.jsx)(n.h4,{id:"batch-load-examples",children:"Batch Load Examples"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Load entire directory from OSS\n./dist/bin/cv load oss://my-bucket/datasets/ \\\n    -c oss.endpoint_url=https://oss-cn-hangzhou.aliyuncs.com \\\n    -c oss.credentials.access_key_id=*** \\\n    -c oss.credentials.access_key_secret=***\n\n# Load from HDFS\n./dist/bin/cv load hdfs://namenode:9000/data/logs/\n"})}),"\n",(0,t.jsx)(n.h3,{id:"configuration-parameters",children:"Configuration Parameters"}),"\n",(0,t.jsx)(n.h4,{id:"s3-compatible-storage-configuration",children:"S3-Compatible Storage Configuration"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Parameter"}),(0,t.jsx)(n.th,{children:"Description"}),(0,t.jsx)(n.th,{children:"Example"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"s3.endpoint_url"})}),(0,t.jsx)(n.td,{children:"S3 service endpoint"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"http://s3.amazonaws.com"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"s3.region_name"})}),(0,t.jsx)(n.td,{children:"S3 region"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"us-east-1"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"s3.credentials.access"})}),(0,t.jsx)(n.td,{children:"Access key"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"***"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"s3.credentials.secret"})}),(0,t.jsx)(n.td,{children:"Secret key"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"***"})})]})]})]}),"\n",(0,t.jsx)(n.h4,{id:"oss-configuration",children:"OSS Configuration"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Parameter"}),(0,t.jsx)(n.th,{children:"Description"}),(0,t.jsx)(n.th,{children:"Example"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"oss.endpoint_url"})}),(0,t.jsx)(n.td,{children:"OSS service endpoint"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"https://oss-cn-hangzhou.aliyuncs.com"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"oss.credentials.access_key_id"})}),(0,t.jsx)(n.td,{children:"Access key ID"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"***"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"oss.credentials.access_key_secret"})}),(0,t.jsx)(n.td,{children:"Access key secret"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"***"})})]})]})]}),"\n",(0,t.jsx)(n.h4,{id:"hdfs-configuration",children:"HDFS Configuration"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Parameter"}),(0,t.jsx)(n.th,{children:"Description"}),(0,t.jsx)(n.th,{children:"Example"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"hdfs.name_node"})}),(0,t.jsx)(n.td,{children:"HDFS NameNode address"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"namenode:9000"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"hdfs.user"})}),(0,t.jsx)(n.td,{children:"HDFS username"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"hadoop"})})]})]})]}),"\n",(0,t.jsx)(n.h3,{id:"hdfs-environment-requirements",children:"HDFS Environment Requirements"}),"\n",(0,t.jsx)(n.p,{children:"Using HDFS functionality requires special compilation and runtime environment configuration:"}),"\n",(0,t.jsx)(n.h4,{id:"compilation-requirements",children:"Compilation Requirements"}),"\n",(0,t.jsx)(n.p,{children:"To enable HDFS functionality, use the special build command with JNI features:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"make build-hdfs\n"})}),"\n",(0,t.jsx)(n.h4,{id:"runtime-environment-requirements",children:"Runtime Environment Requirements"}),"\n",(0,t.jsx)(n.p,{children:"Worker nodes need to configure the following environment:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Java Environment"})," (JDK 8 or above):"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Example: JDK 21 (adjust path according to your actual JDK version)\nexport JAVA_HOME=/usr/lib/jvm/java-21-openjdk-amd64\nexport LD_LIBRARY_PATH=$JAVA_HOME/lib/server:$LD_LIBRARY_PATH\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Hadoop Environment"})," (version should match target HDFS cluster version):"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Example: Hadoop 3.x (adjust path according to your actual Hadoop version)\nexport HADOOP_HOME=/opt/hadoop\nexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\nexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\n\n# Hadoop subsystem environment variables\nexport HADOOP_HDFS_HOME=$HADOOP_HOME\nexport HADOOP_MAPRED_HOME=$HADOOP_HOME\nexport HADOOP_YARN_HOME=$HADOOP_HOME\nexport LD_LIBRARY_PATH=/opt/hadoop/lib/native:$LD_LIBRARY_PATH\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Note"}),": Ensure all Worker nodes have the correct Java and Hadoop environment variables configured, otherwise HDFS mount and load operations will fail."]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}}}]);